package controller

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"syscall"
	"text/template"
	"time"

	"github.com/consensys/linea-monorepo/prover/cmd/controller/controller/metrics"
	"github.com/consensys/linea-monorepo/prover/config"
	"github.com/consensys/linea-monorepo/prover/config/assets"
	"github.com/consensys/linea-monorepo/prover/utils"
	"github.com/sirupsen/logrus"
)

// List of the possible errors returned by the prover as an exit code. There are
// other possibilities but the one listed here are the one of interests. Some of
// these codes are generated by the executor and not the child process itself.
// These includes, CodeFatal, CodeTooManyRetries and CodeCantRunCommand.
const (
	CodeSuccess           int = 0    // Success code
	CodeStdErr            int = 2    // Std. non-zero exit code indicating error and manual investigation
	CodeTraceLimit        int = 77   // The traces are overflown
	CodeOom               int = 137  // When the process exits on OOM
	CodeFatal             int = 14   // When the process could not start
	CodeCantRunCommand    int = 15   // When the controller could not run the command
	CodeCantPreLoad       int = 16   // When the controller could not preload the prover assets
	CodeKilledByExtSig    int = 1137 // When the parent (Controller) process is killed via external signal handlers. We purposefully use a non-Unix code.
	CodePeerAbortReceived int = 1187 // When the child process is killed via SIGUSR2. Relevant only for limitless jobs where one or more of the component failures results in cascading failure.
)

// Status of a finished job
type Status struct {
	// The matched return code
	ExitCode int
	// String explaining what when wrong. "success"
	What string
	// Additional errors for context
	Err error
}

// Resource collects all the informations about the job that can be used to
// fill the input command template. The names of the fields are what defines the
// API of the command templating.
type Resource struct {
	ConfFile string
	// The input and output file paths
	InFile, OutFile string
}

// The executor is responsible for running the commands specified by the jobs
type Executor struct {
	Config *config.Config
	// Logger specific to the executor
	Logger *logrus.Entry
}

func NewExecutor(cfg *config.Config) *Executor {
	return &Executor{
		Config: cfg,
		Logger: cfg.Logger().WithField("component", "executor"),
	}
}

// Run a proof job. Importantly, this code must NOT panic because no
// matter what happens we want to be able to gracefully shutdown. When the
// context is exited, the Run function will SIGKILL the process and the function
// immediately exits.
func (e *Executor) Run(ctx context.Context, job *Job) (status Status) {

	// The job should be locked
	if len(job.LockedFile) == 0 {
		return Status{
			ExitCode: CodeFatal,
			What:     "the job is not locked",
		}
	}

	// We need to spin up a peer abort watcher for limitless prover jobs (ex. inital job bootstrapper)
	// for distributed err propogation and fault tolerance.
	// We exit immediately if one of the components fail. For ex: If the bootstrapper crashes mid-way after
	// writing a few witness and trigerring a few workers, it is of no use for that particular worker job
	// tobe completed. Similarly, if out `n` workers, one of the workers crash because of a genuine failure
	// there is no point for the conglomeration job to wait until the timeout. It can exit immediately. By this
	// way, we save time and compute (cost).
	if job.Def.Name == jobNameConglomeration ||
		strings.HasPrefix(job.Def.Name, jobNameGL) ||
		strings.HasPrefix(job.Def.Name, jobNameLPP) {

		logger := e.Logger.WithField("component", "peer-abort-watcher")
		go PeerAbortWatcher(ctx, job, e.Config, logger)
	}

	// Determine if it's a large job based on the file suffix and config
	// Note: checking that locked job contains "large" is not super typesafe...
	largeRun := job.Def.Name == jobNameExecution && e.Config.Execution.CanRunFullLarge && strings.Contains(job.LockedFile, config.LargeSuffix)

	// Build the initial command (normal for regular jobs, large for large jobs)
	cmd, err := e.buildCmd(job, largeRun)
	if err != nil {
		return Status{
			ExitCode: CodeCantRunCommand,
			Err:      err,
			What:     "can't format the command",
		}
	}

	// Prewarm the assets for limitless jobs
	if err := e.preLoadStaticAssets(ctx, job); err != nil {
		return Status{
			ExitCode: CodeCantPreLoad,
			Err:      err,
			What:     "can't preload the assets",
		}
	}

	// Run the initial command
	status = runCmd(ctx, cmd, job, false)

	// Do not retry for blob decompression, aggregation or conglomeration jobs
	if job.Def.Name == jobNameDataAvailability ||
		job.Def.Name == jobNameAggregation ||
		job.Def.Name == jobNameConglomeration {
		return status
	}

	// If the initial run succeeds, return the status
	if status.ExitCode == CodeSuccess {
		return status
	}

	// If the process has been killed by us (e.g. the context was cancelled),
	// then we also do not retry.
	if status.ExitCode == CodeKilledByExtSig {
		return status
	}

	// Check if the exit code is retryable
	retryableCodes := e.Config.Controller.RetryLocallyWithLargeCodes
	if isIn(status.ExitCode, retryableCodes) {
		if largeRun {
			// For large jobs, retry with the same large command
			status = runCmd(ctx, cmd, job, true)
		} else {
			// For regular jobs, retry with the large command
			largeCmd, err := e.buildCmd(job, true)
			if err != nil {
				return Status{
					ExitCode: CodeCantRunCommand,
					Err:      err,
					What:     "can't format the command",
				}
			}
			status = runCmd(ctx, largeCmd, job, true)
		}
	}

	return status
}

// Builds a command from a template to run, returns a status if it failed
func (e *Executor) buildCmd(job *Job, large bool) (cmd string, err error) {

	// For jobs that produce an actual response file, ensure we can generate
	// the tmp response filename. For jobs that produce no output (e.g. GL/LPP
	// with ResponseRootDir == "/dev/null"), skip ResponseFile generation.
	var outFile string

	if job.Def.ResponseRootDir == "/dev/null" {
		// GL / LPP jobs (or any job explicitly configured to write to /dev/null)
		outFile = "/dev/null"
	} else {
		// The generates a name for the output file. Also attempts to generate the
		// name of the final response file so that we can be sure it will be
		// not fail being generated after having run the command.
		if _, err := job.ResponseFile(); err != nil {
			logrus.Errorf(
				"could not generate the tmp response filename for %s: %v",
				job.OriginalFile, err,
			)
			return "", err
		}
		outFile = job.TmpResponseFile(e.Config)
	}

	var tmpl *template.Template
	switch {
	case job.Def.Name == jobNameExecution || job.Def.Name == jobNameDataAvailability || job.Def.Name == jobNameAggregation:
		tmpl = e.Config.Controller.WorkerCmdTmpl
		if large {
			tmpl = e.Config.Controller.WorkerCmdLargeTmpl
		}
	case job.Def.Name == jobNameBootstrap:
		if e.Config.Controller.ProverPhaseCmd.BootstrapCmdTmpl != nil {
			tmpl = e.Config.Controller.ProverPhaseCmd.BootstrapCmdTmpl
		}
	case job.Def.Name == jobNameConglomeration:
		if e.Config.Controller.ProverPhaseCmd.ConglomerationCmdTmpl != nil {
			tmpl = e.Config.Controller.ProverPhaseCmd.ConglomerationCmdTmpl
		}
	// job.Def.Name looks like "gl-<MODULE>"
	case strings.HasPrefix(job.Def.Name, jobNameGL):
		if e.Config.Controller.ProverPhaseCmd.GLCmdTmpl != nil {
			tmpl = e.Config.Controller.ProverPhaseCmd.GLCmdTmpl
		}
	// job.Def.Name looks like "lpp-<MODULE>"
	case strings.HasPrefix(job.Def.Name, jobNameLPP):
		if e.Config.Controller.ProverPhaseCmd.LPPCmdTmpl != nil {
			tmpl = e.Config.Controller.ProverPhaseCmd.LPPCmdTmpl
		}
	default:
		panic("unknown job type: " + job.Def.Name)
	}

	// use the template to generate the command
	resource := Resource{
		ConfFile: fConfig,
		InFile:   job.InProgressPath(),
		OutFile:  outFile,
	}

	// Build the command and args from the job
	w := &strings.Builder{}
	if err := tmpl.Execute(w, resource); err != nil {
		logrus.Errorf(
			"tried to generate the command for job %s but got %v",
			job.OriginalFile, err,
		)

		// Returns a status indicating that the command templating failed
		return "", err
	}

	// Successfully built the command
	return w.String(), nil
}

// Run a command and returns the status. Retry gives an indication on whether
// this is a local retry or not.
func runCmd(ctx context.Context, cmdStr string, job *Job, retry bool) Status {
	logrus.Infof("The executor is about to run the command: %s", cmdStr)

	// Build exec.Command so we can set process group
	cmd := exec.Command("sh", "-c", cmdStr)

	// Setpgid sets the process group ID of the child to Pgid, or, if Pgid == 0, to the new child's process ID.
	cmd.SysProcAttr = &syscall.SysProcAttr{Setpgid: true}

	// Pipe the child process's stdin/stdout/stderr into the current process
	cmd.Stdin = os.Stdin
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr

	pname := processName(job, cmdStr)
	metrics.CollectPreProcess(job.Def.Name, job.Start, job.End, false)

	startTime := time.Now()
	if err := cmd.Start(); err != nil {
		// Failing to start the process can happen for various different
		// reasons. It can be that the commands contains invalid characters
		// like "\0". In practice, some of theses errors might be retryable
		// and remains to see which one can. Until then, they will need to
		// be manually retried.
		logrus.Errorf("unexpected: failed to start process %v: %v", pname, err)
		return Status{
			ExitCode: CodeFatal,
			What:     "could not start process",
			Err:      err,
		}
	}

	done := make(chan Status)

	go func() {

		// Since the channel is used for sending only once and only in this
		// goroutine, we can safely close it.
		defer close(done)

		// Wait until the process finishes
		_ = cmd.Wait()
		processingTime := time.Since(startTime)
		exitcode, codeErr := unixExitCode(cmd.ProcessState)
		if codeErr != nil {
			logrus.Errorf("error getting unix exit code for %v: %v", pname, codeErr)
			exitcode = CodeFatal
		}

		logrus.Infof(
			"The processing of file `%s` (process=%v) took %v seconds and returned exit code %v",
			job.OriginalFile, pname, processingTime.Seconds(), exitcode,
		)

		// Build the  response status
		status := Status{ExitCode: exitcode}
		switch status.ExitCode {
		case CodeSuccess:
			status.What = "success"
		case CodeOom:
			status.What = "out of memory error"
		case CodeStdErr:
			status.What = "standard error: manual investigation needed"
		case CodeTraceLimit:
			status.What = "trace limit overflow"
		case CodeKilledByExtSig:
			status.What = "received kill signal from controller"
		default:
			status.What = fmt.Sprintf("exit code %d", exitcode)
		}

		metrics.CollectPostProcess(job.Def.Name, status.ExitCode, processingTime, retry)
		done <- status
	}()

	select {

	case <-ctx.Done():

		logrus.Infof("The process %v is being terminated by the controller", pname)
		if cmd.Process != nil {
			pgid := cmd.Process.Pid

			// Set negative id to kill the whole process group and attempt to terminate process gracefully first
			// by sending SIGTERM
			err := syscall.Kill(-pgid, syscall.SIGTERM)
			if err != nil {
				logrus.Warnf("failed to send SIGTERM to process group %v: %v. KILLING it immediately", pgid, err)
				_ = cmd.Process.Kill()
			}
		} else {
			logrus.Warnf("cmd.Process is nil, nothing to kill for %v", pname)
		}

		// Determine if peer abort was signaled
		if peerAbortDetected.Load() {
			return Status{
				ExitCode: CodePeerAbortReceived,
				What:     "the process was requested to be killed by peer abort",
			}
		}

		return Status{
			ExitCode: CodeKilledByExtSig,
			What:     "the process was requested to be killed by the controller (process may not have started)",
		}

	case status := <-done:
		return status
	}
}

// PeerAbortWatcher polls sharedFailureDir for failure-marker files and signals
// the process (SIGUSR2) if a matching marker is found.
// pollInterval is the check interval (e.g. time.Second). If zero, defaults to 1s.
func PeerAbortWatcher(ctx context.Context, job *Job, cfg *config.Config, logger *logrus.Entry) {

	var (
		pollInterval     = time.Duration(cfg.ExecutionLimitless.PollInterval) * time.Second
		sharedFailureDir = cfg.ExecutionLimitless.SharedFailureDir
		failPatternBase  = fmt.Sprintf("%d-%d-*-failure_code*", job.Start, job.End)
		globPattern      = filepath.Join(sharedFailureDir, failPatternBase)
	)

	// Quick initial glob (matches your existing logic)
	files, err := filepath.Glob(globPattern)
	if err != nil {
		logger.Errorf("PeerAbortWatcherPoll: initial glob failed: %v", err)
	} else if len(files) > 0 {
		logger.Warn("PeerAbortWatcherPoll: found abort marker(s) before job start, aborting immediately")
		if err := syscall.Kill(os.Getpid(), syscall.SIGUSR2); err != nil {
			logger.Errorf("PeerAbortWatcherPoll: failed to send SIGUSR2: %v", err)
		}
		return
	}

	logger.Infof("PeerAbortWatcherPoll running for job %v, polling %s for marker %q (interval=%s)",
		job.OriginalFile, sharedFailureDir, failPatternBase, pollInterval)

	ticker := time.NewTicker(pollInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			logger.Infof("PeerAbortWatcherPoll: context done for job %v, exiting", job.OriginalFile)
			return
		case <-ticker.C:
			// glob each tick. Using Glob keeps the same pattern semantics as your original.
			matches, gerr := filepath.Glob(globPattern)
			if gerr != nil {
				// Glob only returns an error if pattern is malformed — shouldn't happen,
				// but log and continue (like your fsnotify version).
				logger.Errorf("PeerAbortWatcherPoll: glob failed: %v", gerr)
				continue
			}
			if len(matches) == 0 {
				// nothing new
				continue
			}

			// For each match, sanity-check that it's present and a regular file
			for _, p := range matches {
				if fi, err := os.Stat(p); err == nil {
					if fi.Mode().IsRegular() {
						logger.Warnf("PeerAbortWatcherPoll: detected failure file %s; sending SIGUSR2", p)
						if err := syscall.Kill(os.Getpid(), syscall.SIGUSR2); err != nil {
							logger.Errorf("PeerAbortWatcherPoll: failed to send SIGUSR2: %v", err)
						}
						return
					}
					// matched but not a regular file (ignore)
				} else {
					// Could be transient — file removed between glob and stat; ignore
					logger.Debugf("PeerAbortWatcherPoll: matched %s but stat failed: %v", p, err)
				}
			}
		}
	}
}

// preLoadStaticAssets: preloads the static prover assets relevant for the job and keeps
// it in the controller mememory and loads it on-demand
func (e *Executor) preLoadStaticAssets(ctx context.Context, job *Job) error {

	// Return immediately if preload is disabled
	if !e.Config.ExecutionLimitless.PreLoadAssets || job.Def == nil {
		return nil
	}

	// Only handle known limitless jobs
	if job.Def.Name != jobNameBootstrap &&
		job.Def.Name != jobNameConglomeration &&
		!strings.HasPrefix(job.Def.Name, jobNameGL) &&
		!strings.HasPrefix(job.Def.Name, jobNameLPP) {
		return nil
	}

	// resolverFn returns (allPaths, criticalPaths, error)
	resolver := assets.NewResolver(e.Config)
	resolverFn := func() ([]string, []string, error) {
		return resolver.AssetsForJob(job.Def.Name)
	}

	// Pass a logger entry for better logs
	logger := e.Logger.WithField("component", "assets-cacher")

	var opts *assets.PreloadOptions
	if job.Def.Name == jobNameConglomeration {
		opts = &assets.PreloadOptions{
			LockFileToRAM: true,
		}
	}

	// Do the once-per-job prefetch (non-blocking risk: it's blocking, but run before spawning child)
	if err := assets.PreloadOnceForJob(ctx, job.Def.Name, resolverFn, opts, logger); err != nil {
		// Consider: if a critical asset is missing, return error to prevent running the job
		logger.WithError(err).Warnf("prefetch once failed for job %s", job.Def.Name)
		// decide policy: here we return error so job doesn't run; you may prefer to continue
		return err
	}

	// Log the residency fraction for every request useful for diagnostics purpose
	assets.LogResMem(job.Def.Name, resolverFn, logger)
	return nil
}

// Returns a human-readable process name. The process name is formatted as in
// the following example: `execution-102-103-<unix-timestamp> <command>`. The
// uuid at the end is there to ensure that two processes never share the same
// name.
func processName(job *Job, cmd string) string {
	return fmt.Sprintf(
		"%v-%v-%v-%v %v",
		job.Def.Name, job.Start, job.End, time.Now().UTC().Unix(), cmd,
	)
}

// Returns true if the x is included in the given list
func isIn[T comparable](x T, list []T) bool {
	for _, y := range list {
		if x == y {
			return true
		}
	}
	return false
}

// Returns the exit code of a process when it exited. The reason for this
// function is that gol std library ExitCode() returns -1 when the process has
// been terminated by a signal. This function prevents that behaviour and
// returns the UNIX exit code. The function returns an error if the process is
// still running. Note that this function will panic on non-UNIX platforms.
func unixExitCode(proc *os.ProcessState) (int, error) {

	waitStatus, ok := proc.Sys().(syscall.WaitStatus)
	if !ok {
		utils.Panic("The controller assumes the underlying process to be UNIX and cannot function properly with the current OS.")
	}

	// Note: here we trust the underlying implementation that if "Exited()",
	// then "ExitStatus()" will return a non-negative value
	if waitStatus.Exited() {
		exitcode := waitStatus.ExitStatus()
		return exitcode, nil
	}

	// Note: "CoreDump" is, in principle, a sub-case of signal receival. But we
	// add a second check just for ease of mind. We trust that the signal is
	// non-negative here.
	if waitStatus.Signaled() || waitStatus.CoreDump() {
		sigCode := waitStatus.Signal()
		return 128 + int(sigCode), nil
	}

	return -1, fmt.Errorf("getting the unix exit code : the process has an unexpected status : %v, it should be terminated", proc.String())
}
